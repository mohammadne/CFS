# History

## Evolution of software deployment (pre-containers era)

- Before containers became mainstream, deploying software was complex, error-prone, and often inconsistent across environments.

1. Bare-Metal Deployments
    Software was installed directly on physical servers.

    Characteristics:
    - No virtualization; each server hosted a single application or service.
    - Tight coupling between application and OS.
    - Manual configuration and installation.

    Challenges:
    - Environment drift: “It works on my machine” problem.
    - Difficult to scale horizontally; provisioning new servers was slow.
    - Hard to isolate applications; a failure in one could affect others.
    - Resource underutilization: servers often idle because they ran only one service.

2. Monolithic Architecture & Manual Dependencies
    - Applications were large monoliths: all features in a single deployable unit.
    - Dependencies (e.g., libraries, database drivers) had to be installed manually.
    - Configuration inconsistencies between environments (dev, test, prod) were common.
    - Rollbacks were difficult: updating the monolith often required a complete reinstall.

3. Virtual Machines (VMs) – Early Abstraction
    Software could run inside a virtualized environment that emulated hardware.

    Key Technologies: VMware ESX, Xen, KVM, Hyper-V.

    Benefits:
    - Multiple VMs on the same hardware improved resource utilization.
    - Isolation between applications via separate guest OSes.
    - Easy snapshot and rollback of entire VMs.

    Limitations:
    - Each VM included a full OS → heavy footprint (GBs per VM).
    - Slower boot times (minutes per VM).
    - VM provisioning and image management were complex.
    - Still required managing OS-level dependencies inside each VM.

4. Configuration Management & Automation Tools
    Tools: Puppet, Chef, Ansible, SaltStack.

    Purpose: Automate software installation, configuration, and updates.

    Improvements:
    - Reduced human error in deployments.
    - Allowed some repeatability across servers.

    Challenges:
    - Still coupled to underlying OS and environment.
    - Complex scripts were hard to maintain.
    - Not fully portable: different OS versions or system packages could break automation.

### Why Containers Became Necessary

- Need for lightweight, reproducible, and portable environments.
- Desire to decouple applications from the host OS.
- Need for fast provisioning and scaling, especially in cloud-native architectures.
- Facilitates DevOps practices: CI/CD pipelines, immutable infrastructure, microservices.

## chroot (1979) → FreeBSD Jails → Solaris Zones → LXC

Linux containerization was built on decades of OS-level isolation concepts. Each step added more isolation, security, and manageability.

1. chroot (1979)
    Definition: “Change root” — changes the apparent root directory for a process.

    Purpose: Isolate a process from the rest of the filesystem.

    How it worked:
    - Process sees a specific directory as /.
    - Process cannot access files outside this root (in theory).

    Limitations:
    - Only filesystem isolation — no CPU, memory, or network control.
    - Processes could escape chroot under certain conditions.
    - No resource limiting, network isolation, or security sandboxing.
    - Impact: First concept of a lightweight “sandboxed” environment.

2. FreeBSD Jails (2000)
    Definition: Lightweight OS-level virtualization in FreeBSD.

    Features:
    - Filesystem isolation (like chroot).
    - Process isolation — each jail has its own PID space.
    - Network isolation — can assign virtual IPs.
    - User isolation — root inside jail is limited to jail.

    Benefits:
    - More secure than chroot.
    - Multiple jails could run on the same kernel without interfering.

    Limitations:
    - Only works on FreeBSD.
    - Still shares the same kernel — no kernel-level isolation.

3. Solaris Zones (2004)
    Definition: OS-level virtualization for Solaris (now Oracle Solaris).

    Types:
    - Global Zone: The main OS environment.
    - Non-global Zones: Isolated virtual environments inside the same kernel.

    Features:
    - Filesystem, process, and network isolation.
    - Resource controls via projects and resource pools.
    - Ability to allocate CPU, memory, and device access per zone.

    Benefits:
    - Strong isolation while sharing a single OS kernel.
    - Can run multiple applications in isolated environments efficiently.

    Limitations:
    - Only available on Solaris.
    - Limited portability outside the Solaris ecosystem.

4. Linux Containers (LXC) (2008)
    Definition: OS-level virtualization using Linux kernel features.

    Key Kernel Features Used:
    - Namespaces: PID, NET, MNT, IPC, UTS, USER.
    - cgroups: CPU, memory, I/O resource limits.
    - Union/overlay filesystems: Layered filesystem support.

    Features:
    - Full process isolation.
    - Resource limiting.
    - More portable than previous jails/zones.

    Benefits:
    - Lightweight — near-native performance.
    - Multi-application isolation on Linux hosts.
    - Precursor to Docker’s container runtime.

    Limitations:
    - Management was more complex than Docker.
    - No standard image format; LXC required manual configuration.

## The rise of Linux container primitives

1. Linux Namespaces (2002–2007)
    Definition: Kernel feature that isolates various aspects of a process’s environment.
    Types of namespaces:
    PID namespace: Isolates process IDs — processes in one namespace cannot see or affect others.
    NET namespace: Isolates network interfaces, IP addresses, routing tables, and firewall rules.
    MNT namespace: Isolates filesystem mount points.
    IPC namespace: Isolates System V IPC objects and POSIX message queues.
    UTS namespace: Isolates hostnames and domain names.
    USER namespace: Maps user and group IDs, enabling rootless containers.
    Impact: Allowed each container to appear as a separate system, giving the illusion of a full OS environment.

2. Control Groups (cgroups) (2007)
    Definition: Kernel feature that limits, accounts for, and isolates resource usage of process groups.
    Capabilities:
    CPU: Limit CPU shares or enforce quotas.
    Memory: Set maximum memory usage; prevent runaway processes.
    I/O: Control disk I/O bandwidth.
    Tasks: Track and manage process hierarchies.
    Impact: Provided resource management, ensuring containers could coexist on the same host without interfering with each other.

3. Union/Overlay Filesystems
    Definition: File systems that allow multiple layers to appear as a single filesystem.
    Examples: OverlayFS, AUFS.
    Benefits for containers:
    Efficient storage by sharing base layers among multiple containers.
    Copy-on-write: Changes are stored in a new layer, leaving base layers immutable.
    Faster image builds and smaller storage footprint.

## Docker’s launch (2013) and why it changed everything

Docker introduced a new way to package, ship, and run applications by combining Linux container primitives with a developer-friendly workflow

### The Problem Before Docker

Before Docker, even with LXC or OpenVZ, developers faced:
Complex container management (manual LXC configs).
Lack of standard image formats; images were not portable.
Hard-to-reproduce environments across development, testing, and production.
VM-based deployments were heavy and slow.
CI/CD pipelines were difficult to implement reliably.

### Docker’s Innovations

Docker built on top of Linux container primitives (namespaces, cgroups, overlayfs) and introduced several game-changing ideas:

1. Standardized Container Images
    - Docker images: Portable, immutable, and versioned.
    - Layers & caching: Each layer represents changes; builds are faster and storage-efficient.
    - Reproducibility: “Works on my machine” problem solved.

2. Developer-Friendly CLI & Workflow
    - docker build, docker run, docker pull, docker push.
    - Simple syntax for building and running containers.
    - Easy experimentation for developers without deep Linux kernel knowledge.

3. Docker Hub – The Public Registry
    - Central repository to share and distribute container images.
    - Facilitates reuse of official base images (Ubuntu, Alpine, Python, Node, etc.).
    - Enables collaboration and faster project onboarding.

4. Multi-Platform Support
    - Docker abstracts away OS differences using images.
    - Works consistently across Linux, macOS (via Docker Desktop), and Windows.
    - Accelerated adoption in heterogeneous development environments.

5. Lightweight & Fast
    - Containers share the host kernel; near-native performance.
    - Boot in seconds vs minutes for VMs.
    - Efficient resource utilization enables high-density deployments.

## Standardization: OCI (Open Container Initiative)

The Open Container Initiative (OCI) was created to standardize container formats and runtimes, ensuring portability, interoperability, and longevity of container technology across platforms and vendors.

### Why OCI Was Needed

Docker popularized containers, but early container images and runtimes were proprietary:
Different vendors had incompatible image formats.
Runtimes and tools were fragmented.
Lack of standards threatened portability and ecosystem growth.
Goal: Define open, vendor-neutral specifications for container images and runtimes.

Formation
Year: 2015
Founding Members: Docker, CoreOS, Red Hat, Google, Microsoft, and other industry leaders.

So, OCI is A set of standards (specifications) for containers.

### OCI Specifications

OCI defines two primary specifications:

3.1 OCI Runtime Specification (runtime-spec)
Defines how a container should be started and run on any compliant runtime.
Specifies:
Process lifecycle
Namespaces and cgroups usage
Filesystem mounting
Hooks and lifecycle events
Example runtime: runc implements this spec.

3.2 OCI Image Specification (image-spec)
Defines how container images are built, stored, and distributed.
Includes:
Image manifest
Layer structure
Metadata (e.g., creation time, author)
Ensures images built with one tool run in any OCI-compliant runtime.

### How They Work Together

You type `docker run my-app`.

1. Docker talks to containerd to:
    - Pull the image
    - Manage its layers
    - Prepare filesystem and resources

2. containerd calls runc to:
    - Start the process
    - Apply isolation (namespaces)
    - Limit resources (cgroups)
    - The container starts, fully isolated, and running your app.

+---------------------+
|       Docker CLI     |  <-- You
+---------------------+
          |
          v
+---------------------+
|      containerd      |  <-- Manages images & lifecycle
+---------------------+
          |
          v
+---------------------+
|        runc         |  <-- Runs container with isolation
+---------------------+
          |
          v
+---------------------+
| Linux Kernel        |  <-- Namespaces, cgroups, overlayFS
+---------------------+

## Impact on DevOps, CI/CD, cloud-native ecosystem

Docker didn’t just introduce containers; it transformed the way software is developed, tested, deployed, and scaled.
