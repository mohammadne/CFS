# Docker

## How Docker Works

Docker has a client-server architecture:

```txt
docker CLI (client)  →  Docker Daemon (dockerd)
```

- The CLI sends requests to dockerd via a Unix socket (/var/run/docker.sock)
- The server actually performs actions like pulling images, building, and running containers.

```sh
ls -l /var/run/docker.sock
```

When pulling an image, under the hood:

`CLI → dockerd → containerd → snapshotter → registry`

```sh
docker pull alpine:3.22.2

docker image inspect alpine:3.22.2 --format '{{json .RootFS.Layers}}' | jq
```

- Each layer = a snapshot in overlayfs
- Layers are cached → reused across containers

### Build, run, pull, push flow

```dockerfile
FROM alpine:3.22.2
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
```

Under the Hood:

- Docker parses each Dockerfile instruction
- Each instruction → new layer (overlayfs snapshot)
- Layers are stored via containerd snapshotter
- BuildKit uses cache to skip unchanged steps

runc sets up:

- Namespaces (PID, NET, MNT, IPC, UTS, USER)
- cgroups (CPU, Memory, IO limits)
- OverlayFS mounts for the filesystem

```sh
# build image
docker build -t myapp:latest .

# Inspect layers
docker history myapp:latest

# Running container
docker run -d --name mynginx -p 8080:80 nginx

# Example: see container PID namespace
docker inspect mynginx --format '{{.State.Pid}}'

# Inspect namespaces manually
lsns | grep $(docker inspect xray-notls --format '{{.State.Pid}}')
```

## Docker Image Architecture & Layer Caching

### Image manifest & Layer structure

Docker images are built from layers

```dockerfile
FROM python:3.12-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY . .
CMD ["python", "app.py"]
```

Layers created:

1. FROM python:3.12-slim → base image layer
2. WORKDIR /app → metadata layer
3. COPY requirements.txt . → adds file layer
4. RUN pip install ... → installs dependencies layer
5. COPY . . → app code layer
6. CMD [...] → config layer

```sh
# .RootFS.Layers lists sha256 hashes of each layer
docker inspect nginx:latest --format '{{json .RootFS.Layers}}' | jq

# explain the layers
docker history myapp:latest
```

### Caching behavior in builds

- Docker caches layers to reduce rebuild time
- Only layers after changes are rebuilt

### Reproducibility & determinism

builds may differ like this:

```sh
# non-deterministic
RUN apt-get update && apt-get install -y python3

# deterministic
RUN apt-get update && apt-get install -y python3=3.12.0-1
```

### Multi-stage builds

Multi-stage builds separate build environment from final runtime, reducing final image size.

```dockerfile
# Stage 1: Build
FROM golang:1.21 AS builder
WORKDIR /app
COPY . .
RUN go build -o myapp .

# Stage 2: Runtime
FROM alpine:3.22.2
WORKDIR /app
COPY --from=builder /app/myapp .
CMD ["./myapp"]
```

Benefits:

- Only the final binary is included → smaller image
- Build dependencies (golang SDK) not shipped
- Layers for the build stage are not carried over

Inspect final image size:

```sh
docker images
```

## Understanding the OCI Ecosystem

### containerd

A daemon/executable managing container lifecycle.

Handles:

- Pulling/pushing images
- Snapshotting (filesystem layers)
- Launching containers via runc
- Used internally by Docker and Kubernetes.

### runc

Lightweight executable that actually starts a container.

Responsibilities:

- Setup namespaces (PID, NET, MNT…)
- Setup cgroups (CPU, memory, I/O limits)
- Launch the container process (PID 1)

### CRI: Kubernetes runtime interface

Kubernetes API for container runtimes.

Allows Kubernetes to be runtime-agnostic:

- Docker (via dockershim, deprecated)
- containerd
- CRI-O

### CNI: networking interface

Standard for connecting containers to networks.

Handles:

- Bridge, overlay, macvlan, etc.
- IPAM (IP address assignment)
- DNS & routing

### ORAS: OCI registry artifacts (non-container files)

OCI registry standard for storing any files, not just container images.

Examples:

- Helm charts
- WASM binaries
- SBOMs (Software Bill of Materials)

## OCI & Runtime Architecture

1. OCI Image Spec

Defines:
How an image is structured
Layers, manifests, configs
Digest/sha256 for immutability
All registries and runtimes follow this.

2. OCI Runtime Spec
Defines:
How to launch a container
Process lifecycle
cgroups & namespaces configuration
JSON bundle format (config.json)

3. runc (Low-Level Runtime)

Executes containers using namespaces + cgroups
Created by Docker, donated to OCI
Very small, secure, standard
runc = the thing that actually starts the process inside a namespace/cgroup.

4. containerd

> It's a container runtime daemon written in Go which provides APIs for pulling images, managing snapshots, and running containers.

Handles:
Image pull/push
Snapshotters (OverlayFS, btrfs, zfs)
Managing containers
Exposing gRPC API
Supervised by Docker or used standalone
Default runtime for Kubernetes (via CRI)

containerd = brains
runc = executor

Engines uses containerd:
Docker
Kubernetes (directly)
NerdCTL

Engines does NOT use containerd:
Podman
CRI-O

5. CRI (Container Runtime Interface)

Used by Kubernetes.
Kubernetes ↔ CRI ↔ containerd
or
Kubernetes ↔ CRI ↔ CRI-O
K8s does not talk to Docker directly anymore.
