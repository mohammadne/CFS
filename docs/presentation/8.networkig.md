# Networking

## Docker Networking Overview

Docker networking is the system that enables containers to communicate with each other, the host, and external networks.
It defines how data moves between containers and across systems during containerized application execution.

It provides isolated, flexible network environments using built-in drivers like bridge, host, overlay, and none.
Each driver supports different use cases, such as local development, swarm-based orchestration, or integration with legacy infrastructure.

## Key networking concepts

Below are the key concepts related to Docker networking:

- `Network namespaces` – Each Docker container runs inside its own network namespace, isolating its network stack from the host and other containers. This ensures that IP addresses, routing tables, and interfaces don’t conflict across containers.

- `Virtual Ethernet interfaces (veth pairs)` – Docker uses veth pairs to connect containers to networks. One end is inside the container, and the other connects to a bridge or another network device on the host.

- `Bridges` – A bridge acts like a virtual switch that forwards traffic between containers and the host. Docker automatically creates a default bridge to connect containers unless specified otherwise.

- `NAT (Port mapping)` – Containers can expose specific ports to the host using port mapping. This allows external clients to access containerized applications through the host’s IP and a designated port.

- `DNS Resolution` – Docker provides an internal DNS service that allows containers to resolve each other by name. This makes service discovery easier without needing to hardcode IP addresses.

- `Subnets and IP addressing` – Docker assigns containers IP addresses from configured subnets. This allows containers to communicate directly with each other within a network.

- `Routing` – Docker manages routing rules so that packets can move between containers, the host, and external networks. Containers follow the routing table defined by their network namespace.

- `Firewall` rules (iptables) – Docker configures iptables rules to manage traffic between containers, hosts, and external networks. These rules enforce isolation and port forwarding.

### Docker Networks

1. Bridge Network (Default)

- Each container gets a private IP on the bridge subnet (default: 172.17.0.0/16).
- Host performs NAT to allow external access.
- Containers can communicate using container name inside the bridge network.

Bridge networks create a software-based bridge between your host and the container. Containers connected to the network can communicate with each other, but they’re isolated from those outside the network.

Each container in the network is assigned its own IP address. Because the network is bridged to your host, containers can also communicate on your LAN and the Internet. However, they will not appear as physical devices on your LAN.

```sh
# Create a custom bridge
docker network create \
--driver bridge \
--attachable \
--scope local \
--subnet 10.0.42.0/24 \
--ip-range 10.0.42.128/25 \
user-network

docker run -d \
--network user-network \
--name network-explorer1 \
alpine:3.22.2 sleep 3600

docker run -it \
--network user-network \
--name network-explorer2 \
alpine:3.22.2 \
sh

ip -f inet -4 -o addr

# Run containers attached to it
docker run -d --name app1 --network mybridge nginx
docker run -d --name app2 --network mybridge nginx

# Containers can ping each other by name
docker exec -it app1 ping app2
```

2. Host Network

- Container uses host’s network namespace.
- No port mapping needed.
- Performance is better (no NAT).

```sh
docker run --network host nginx
```

- No isolation from host → security risk
- Cannot run multiple containers binding to same port

3. none

Disables networking completely. Useful for security or manual configuration.

The none network type in Docker disables all networking for a container. It prevents the container from being connected to any external network, including the default bridge network.

This means that no network interface besides lo (loopback) is created, and the container cannot communicate with other containers or the host. It’s commonly used for containers that don’t need network access, such as isolated tasks or for enhanced security.

4. overlay

Enables multi-host networking using Docker Swarm. It creates a distributed network across nodes, allowing containers on different hosts to communicate securely.

Overlay networks are distributed networks that span multiple Docker hosts. The network allows all the containers running on any of the hosts to communicate with each other without requiring OS-level routing support.

Overlay networks implement the networking for Docker Swarm clusters, but you can also use them when you’re running two separate instances of Docker Engine with containers that must directly contact each other. This allows you to build your own Swarm-like environments.

5. macvlan

Assigns a MAC address to each container, making it appear as a physical device on the network. Used for scenarios requiring full network integration, such as legacy apps.

6. ipvlan

Similar to macvlan but uses a different method for traffic handling. It’s more efficient for high-density environments but less flexible.

## Networking Inside Containers

When a container is started, it gets its own isolated network environment.

Docker uses Linux kernel primitives to provide container networking, allowing isolation, communication, and external access.

### Network namespaces

Every container runs in its own network namespace. a network namespace provides a separate networking stack, including:

- IP addresses
- Routing tables
- Interfaces (eth0, lo)
- Firewall rules

### veth pairs

veth pairs act like a virtual wire connecting the container’s network namespace to the host (or a bridge).

```txt
container eth0 → veth pair → docker bridge → host/other container
```

### Linux bridges & NAT (iptables)

Docker uses a Linux bridge (like a virtual switch) to connect containers in the same network.

Default bridge: docker0 on host.

NAT (iptables) allows containers to access external networks.

- Containers get private IPs (172.17.0.0/16)
- NAT maps container IP → host IP for outgoing traffic

### DNS resolution (embedded DNS)

### Routing basics inside containers

```sh
docker network create --subnet=192.168.5.0/24 mynet
docker run --network mynet --rm busybox ip route

lsns | grep $(docker inspect my-web-server --format '{{.State.Pid}}')
```

```txt
Container eth0
    │
    ▼
veth pair → Host bridge (docker0)
    │
    ├─ To other container (same bridge)
    └─ NAT → Host → Internet
```

```sh
# Show all Docker networks
docker network ls

# Inspect a specific network (default: bridge)
docker network inspect bridge

# inspect its IP
docker run -d --name c1 busybox sleep 3600
docker inspect -f c1
```

Exec into the container and check networking inside

```sh
docker exec -it c1 sh
# inside container:
ip addr
ip route
cat /etc/resolv.conf
```

Connect two containers and ping them

```sh
docker run -d --name c2 busybox sleep 3600
docker exec c1 ping -c 3 c2
# fails (default bridge network does NOT provide DNS-based container name resolution)

docker rm -f c1 c2
docker network prune
```

Create a user-defined bridge and attach containers

```sh
docker network create mynet
docker run -d --name c1 --network mynet busybox sleep 3600
docker run -d --name c2 --network mynet busybox sleep 3600

docker network inspect mynet

docker exec c1 ping -c 3 c2
docker exec c2 ping -c 3 c1

docker exec c1 nslookup c2

docker rm -f c1 c2
docker network prune
```

NATing

```sh
docker run -d --name c3 \
  -p 8080:8000 \
  nicolaka/netshoot \
  python3 -m http.server 8000

curl http://localhost:8080

# see DNAT rules created by Docker
sudo iptables -t nat -L -n | grep 8080
# tcp dpt:8080 to:172.17.0.2:8000

# on host
ip link | grep veth
# veth1234 → container eth0

docker exec c3 ip route
# default via 172.17.0.1 dev eth0
```

Demonstrate “no networking”

```sh
docker run --rm -it --network none nicolaka/netshoot sh

ping google.com  # fails
```
